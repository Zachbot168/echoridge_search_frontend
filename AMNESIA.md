# AMNESIA.md - EchoRidge Search Integration Context

## ğŸ¯ CRITICAL: Complete System Integration

This frontend is now **fully integrated** with the backend pipeline via production-grade services:

```
PMF Pipeline â†’ JSONL Files â†’ ETL Service â†’ PostgreSQL â†’ Catalog API â†’ This Frontend
```

## ğŸ¯ Core Product Philosophy

This is a **universal catalog search application** backed by real business intelligence data:

1. **Universal Catalog**: All users query the same high-quality dataset from PMF Finder pipeline
2. **Real Data**: Connected to actual business discovery, web scraping, and AI scoring pipeline
3. **Production Ready**: ETag caching, cursor pagination, deterministic IDs, audit trails
4. **User Overlays**: Personal data (bookmarks, notes) stored locally, never mixed with global data

## ğŸ—ï¸ Architecture Overview - DUAL MODE INTEGRATION

### Mode 1: Live Pipeline Execution (Primary)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Frontend (This Repo)                           â”‚
â”‚  Query + Geofence â†’ Pipeline API â†’ PMF Backend â†’ Fresh Results       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ Search Request
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Pipeline API (FastAPI)                            â”‚
â”‚  /v1/pipeline/execute  /v1/pipeline/status  /v1/pipeline/results      â”‚
â”‚  Triggers PMF backend execution with geofencing support              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ Execute CLI Command
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PMF Finder Pipeline (Backend)                      â”‚
â”‚  CLI â†’ Google Places â†’ Web Scraping â†’ AI Scoring â†’ JSONL Files       â”‚
â”‚  Input: query, bbox/center, max_results, provider                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ Fresh Business Data
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend Cache (SQLite WASM)                      â”‚
â”‚  Cache results locally for fast repeat access and offline support    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Mode 2: Historical Catalog Access (Secondary)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PMF Finder Pipeline (Backend)                      â”‚
â”‚  CLI â†’ Google Places â†’ Web Scraping â†’ AI Scoring â†’ JSONL Files       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ Historical Raw Business Data
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ETL Service                                   â”‚
â”‚  Transform JSONL â†’ PostgreSQL with deterministic IDs & checksums     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ Normalized Historical Catalog
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Catalog API (FastAPI)                             â”‚
â”‚  /v1/catalog/companies  /v1/catalog/evidence  + ETag/Cursor support  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ Historical Data Access
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend (This Repo)                              â”‚
â”‚  Browse historical data, compare with fresh pipeline results         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”‘ Critical Concepts - UPDATED FOR INTEGRATION

### 1. Data Flow: Real Pipeline Integration

**Backend Pipeline (pmf_finder_backend/)**:
- Input: `"Private schools in Tampa"`
- Output: `places_norm.jsonl`, `scores.jsonl`, `scrapes.jsonl`
- Content: Real Google Places data, GPT-4 DIMB scores, web scraping

**ETL Service (services/etl/)**:
- Input: Backend JSONL files
- Process: Generate deterministic UUIDs, map DIMBâ†’DOIMB, validate checksums
- Output: PostgreSQL catalog with companies, evidence, geofences

**Catalog API (services/api/)**:
- REST endpoints matching frontend expectations
- ETag caching, cursor pagination, production error handling
- Real data serving with <100ms response times

### 2. Universal Catalog vs User Overlays - LIVE DATA

**Universal (Real Business Data)**:
- `companies` - Actual businesses from Google Places + AI scoring
- `evidence` - Real web content from Firecrawl scraping
- `geofences` - Geographic boundaries from pipeline queries
- Scoring: D/I/M/B from GPT-4 analysis of real websites
- Determinism: Full audit trail with checksums and context versions

**User Overlays (Per-User/Org)**:
- `workspace_bookmarks` - Saved companies
- `workspace_comparisons` - Side-by-side analysis
- `workspace_notes` - Personal annotations
- `search_history` - Recent queries

### 3. ID Strategy - IMPLEMENTED DETERMINISTIC SYSTEM
- **Deterministic UUIDs**: ETL generates UUID v5 from `domain:example.com` or `name:Company|addr:123 Main St`
- **Global Consistency**: Same company always gets same `global_company_id` across runs
- **No Local Generation**: Frontend only consumes, never creates IDs
- **Referential Integrity**: All overlays use `global_company_id` foreign keys

### 4. Scoring Determinism - PRODUCTION AUDIT TRAILS
Every score includes complete lineage:
- `norm_context_version`: "placenorm@v2|dimb@v1" (tracks ETL and AI model versions)
- `checksum`: SHA256 of canonical company JSON (detects any data changes)
- `confidence_score`: From Google Places API confidence + AI model certainty
- Full D/O/I/M/B breakdown: Real GPT-4 analysis with reasoning
- Audit trail: Links to original web scrapes and raw JSONL sources

## ğŸŒ Environment Configuration - DUAL API SETTINGS

```bash
# Live Pipeline API (Triggers fresh searches)
NEXT_PUBLIC_PIPELINE_API_BASE_URL=http://localhost:8082

# Historical Catalog API (Browse past results)
NEXT_PUBLIC_API_BASE_URL=http://localhost:8081

# Multi-tenant Mode (Universal catalog model)
NEXT_PUBLIC_TENANT_MODE=universal        # All users see same data
NEXT_PUBLIC_AUDIENCE_SCOPE=public        # Can be 'partner', 'enterprise'

# Database Driver (SQLite WASM for caching)
NEXT_PUBLIC_DB_DRIVER=sqlite-wasm        # Production, fallback to 'indexeddb'
```

### API Service Ports
- **Pipeline API**: Port 8082 - Executes fresh PMF searches with geofencing
- **Catalog API**: Port 8081 - Serves historical data from PostgreSQL catalog
- **Frontend**: Port 3000-3003 - Next.js development server

## ğŸ“Š Database Schema

### Global Tables (Read-Only from API)
```sql
-- Companies with global IDs and scoring
CREATE TABLE companies (
    global_company_id TEXT PRIMARY KEY,  -- From API, never generated
    dataset_version TEXT,                -- e.g., "Catalog v2025.09"
    name TEXT NOT NULL,
    domain TEXT,
    -- PMF Finder fields
    postal_code TEXT,
    country_code TEXT,
    -- Echo Ridge scoring
    final_score REAL,
    d_score REAL,
    o_score REAL,
    i_score REAL,
    m_score REAL,
    b_score REAL,
    confidence_score REAL,
    norm_context_version TEXT,           -- Critical for determinism
    checksum TEXT,                       -- Critical for reproducibility
    -- Metadata
    updated_at TEXT,
    synced_at TEXT
);

-- Evidence pointers (not full documents)
CREATE TABLE evidence (
    evidence_id TEXT PRIMARY KEY,
    global_company_id TEXT,
    type TEXT,
    title TEXT,
    preview TEXT,                        -- Lightweight preview only
    source_url TEXT,
    created_at TEXT
);
```

### User Overlay Tables (Local/Syncable)
```sql
-- User bookmarks reference global IDs
CREATE TABLE workspace_bookmarks (
    user_id TEXT NOT NULL,
    org_id TEXT,                         -- Optional for multi-org
    global_company_id TEXT NOT NULL,
    created_at TEXT,
    updated_at TEXT,
    UNIQUE(user_id, global_company_id)
);
```

## ğŸ”„ Sync Strategy

### Phase 1: Hydrate Global Catalog
1. Check ETags for each resource type
2. Fetch changed companies with cursor pagination
3. Preserve ALL scoring metadata and determinism fields
4. Update `sync_state` table with last ETags

### Phase 2: Materialize Facets
1. Compute facet buckets from local data
2. Store in `facet_buckets` for instant filtering

### Phase 3: Load User Overlays
1. Check local storage first
2. Optionally sync with Supabase user store
3. Never mix with global data

### Failure Handling
- If API unreachable: Use last good cache, show "stale" badge
- Overlays remain fully functional offline
- Never lose determinism metadata

## ğŸš€ Performance Targets

- **Warm search**: â‰¤150ms (from cache)
- **Cold search**: â‰¤350ms (with DB init)
- **Facet toggle**: Instant (pre-bucketed)
- **50k+ companies**: Must handle smoothly

## âš ï¸ Critical Rules

1. **NO SERVER WRITES** - This frontend never mutates catalog data
2. **PRESERVE DETERMINISM** - Never drop checksums or norm_context_version
3. **RESPECT UNIQUENESS** - Follow PMF Finder rules (domain OR name+postal+country)
4. **NO DATA FORKING** - User edits go in overlays only, never duplicate global rows
5. **GLOBAL IDS ONLY** - Never generate local IDs for catalog entities

## ğŸ“ Key Files to Implement

```
lib/
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ engine.ts        # SQLite WASM initialization âœ…
â”‚   â”œâ”€â”€ schema.sql       # Complete DDL âœ…
â”‚   â”œâ”€â”€ migrate.ts       # Idempotent migrations âœ…
â”‚   â”œâ”€â”€ sync.ts          # API sync orchestration âœ…
â”‚   â”œâ”€â”€ query.ts         # Query API for UI âœ…
â”‚   â””â”€â”€ parse.ts         # Zod validation schemas âœ…
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ client.ts        # HTTP client with ETag/retry âœ…
â”‚   â””â”€â”€ endpoints.ts     # Typed API definitions âœ…
â””â”€â”€ types/
    â””â”€â”€ db.ts            # TypeScript interfaces âœ…
```

All core infrastructure has been implemented AND INTEGRATED with real backend data.

## ğŸ”® Future Considerations

- **Multi-tenant deltas**: If backend adds per-tenant enrichments, sync layer must support overlay streams
- **Offline-first**: Current architecture supports full offline with stale data warnings
- **User sync**: Overlay tables ready for Supabase auth integration

## ğŸ­ State Management

The app maintains three levels of state:
1. **Global Catalog State** - Immutable, from API
2. **User Overlay State** - Mutable, local-first
3. **UI State** - Ephemeral (selections, modals, etc.)

Never let these mix - maintain clear boundaries.

## ğŸ” Query Patterns

```typescript
// Always query global tables for catalog data
searchCompanies({ q, filters, sort }) // â†’ queries companies + facet_buckets

// Overlay queries always join to global
getBookmarkedCompanies(userId) // â†’ JOIN workspace_bookmarks to companies

// Never duplicate data
addBookmark(userId, globalCompanyId) // â†’ INSERT to overlay only
```

## ğŸ“ Testing Considerations

1. Mock API responses must include all determinism fields
2. Test overlay isolation (user data doesn't leak)
3. Verify ETag caching behavior
4. Test offline scenarios with stale data
5. Ensure global IDs are never generated locally

---

## ğŸš€ DEPLOYMENT INSTRUCTIONS

### Quick Start (Complete Pipeline)
```bash
# From project root
./scripts/setup-pipeline.sh
```

### Manual Deployment
```bash
# 1. Start core services
docker-compose -f docker-compose.catalog.yml up catalog-db catalog-api pipeline-api -d

# 2. Transform backend data (optional - for historical catalog)
docker-compose -f docker-compose.catalog.yml run --rm catalog-etl python main.py ingest --latest

# 3. Start frontend
docker-compose -f docker-compose.catalog.yml up catalog-frontend -d

# 4. Access
# Frontend: http://localhost:3000 (for live pipeline execution)
# Pipeline API: http://localhost:8082 (triggers fresh searches)
# Catalog API: http://localhost:8081 (historical data browsing)
# API Docs: http://localhost:8082/docs (Pipeline), http://localhost:8081/docs (Catalog)
```

### Development Mode
```bash
# Pipeline API (live search execution)
cd services/pipeline && python main.py  # Starts on localhost:8082

# Catalog API (historical data)
cd services/api && python main.py       # Starts on localhost:8081

# Frontend (with all controls)
cd echoridge_search_frontend && npm run dev  # Starts on localhost:3000+

# ETL operations (for historical catalog)
cd services/etl && python main.py --help

# Test PMF backend directly
cd pmf_finder_backend && python run_pipeline.py --query "restaurants in Austin" --max 10
```

## âš ï¸ CRITICAL SUCCESS REQUIREMENTS

1. **Backend Data Must Exist**: Run PMF pipeline first to generate JSONL files
2. **ETL Must Complete**: Transform data before frontend shows results
3. **API Must Be Healthy**: Check `/health` endpoint before frontend
4. **Environment Variables**: Set API URL in frontend `.env`

---

## ğŸ¯ INTEGRATION COMPLETE

This frontend is now a **production-grade catalog search interface** with:

âœ… **Real Business Data**: Connected to Google Places + AI scoring pipeline
âœ… **Live API Integration**: FastAPI with ETag caching and pagination
âœ… **Production Database**: PostgreSQL with deterministic IDs
âœ… **Performance**: <350ms search with SQLite WASM caching
âœ… **Audit Trails**: Complete determinism and reproducibility

Remember: This is a **real-time window** into a **live business intelligence catalog**. User personalizations live in **overlays** that reference (never duplicate) the global truth.